{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy GPT Model \n",
    "A GPT arhitecture contains embedding + positional encoding --> normalization --> some other operations in between --> transformer (scaled dot product attention in multi head attention + masked attention) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3099, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3303, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3486, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3546, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/7j/w7_7w_8x2l71rf18ty9xsp8w0000gn/T/ipykernel_16306/1944201880.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/newEnv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403226120/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone Terms: \n",
    "- vocabulary size: total # of vocabularies in the training dataset (depends on tokenization, we may add other <> tokens as well). \n",
    "- context_length: max number of input tokens taken at each time \n",
    "- embed_dim: the dimension of the word embedding at encoding stage (more dim can improve words learning better)\n",
    "- n_layers: number of hidden layers in the transformer block \n",
    "- n_heads: # of attention heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    \"vocab_size\" : 50257, \n",
    "    \"context_length\" : 1024, \n",
    "    \"embed_dim\" : 768,\n",
    "    \"n_heads\" : 12, \n",
    "    \"n_layers\" : 12, \n",
    "    \"drop_rate\" : 0.1  # each node during the hidden layer has 10% being dropped. Avoids overfitting \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__() # init torch.nn.Module class \n",
    "        self.embed= nn.Embedding(cfg[\"vocab_size\"], cfg[\"embed_dim\"])\n",
    "        self.pos_embed= nn.Embedding(cfg[\"context_length\"], cfg[\"embed_dim\"])   # add the positional encoding. \n",
    "        # input will be the same as the context length (# of tokens processed each time, and embed them at a hidden dimension)\n",
    "\n",
    "        self.drop_embed= nn.Dropout(cfg[\"drop_rate\"])   # drop out function \n",
    "        #TODO: self.transformer = nn.Sequential () # transformer block \n",
    "        #TODO: self.final_norm= LayerNorm (cfg[\"embed_dim\"])   # layer normalization \n",
    "        self.out_head= nn.Linear(cfg[\"embed_dim\"], cfg[\"vocab_size\"], bias=False)   # final FC layer back to vocabulary\n",
    "\n",
    "    # return the non-softmax output of \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len= in_idx.shape # vector would be importe as the size batch x vocabulary size/ length of the setence\n",
    "        text_embed= self.embed(in_idx)\n",
    "        pos_embed= self.pos_embed(torch.arange(seq_len, device= in_idx.device)) # non-repeating positional encoding (copies the entire weight matrix of the token_embed)\n",
    "        \n",
    "        # step 1: add embedding and positional encoding \n",
    "        x= pos_embed + text_embed \n",
    "        # step 2: apply drop out \n",
    "        x= self.drop_embed(x)\n",
    "        # step 3 :go through the transformer \n",
    "        x= self.transformer(x) \n",
    "        # step 4: more layer normalization and then FC to output \n",
    "        x= self.final_norm(x) \n",
    "        logits= self.out_head(x) \n",
    "        return logits \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizing the input as example \n",
    "- Tokenize input with pre built tokenzier \n",
    "    - tokenzier will break down the input into tokens then make them into corresponding one-hot vector representations \n",
    "- Add to batch to feed into GPT model (the batch will be out in_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []  # append all text \n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization \n",
    "Used to reset mean to 0 and variance to 1, so the training process is stable and we are less likely to hit convergence earlier and more effective   \n",
    "Normalized Vector $$\\frac{(x-\\mu)}{\\sigma}$$  \n",
    "- Sometimes the normalization involves calculating the variance, and the formula of variance is $\\sum{\\frac{(X-\\mu)^2}{N}}$ because we divide by $N$, this doesn't use **Bessel's correction**, which would construct a non-biased variance by diving by $N-1$ instead of $N$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4490, 0.4633],\n",
      "        [0.4363, 0.3558]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.4561],\n",
      "        [0.3960]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0001],\n",
      "        [0.0032]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "a= torch.randn(2,5)\n",
    "layer= nn.Sequential(nn.Linear(5,2), nn.ReLU())\n",
    "out= layer(a) \n",
    "print(out)\n",
    "\n",
    "# compute the mean and variance of these two tensors \n",
    "mean= out.mean(dim=-1, keepdim=True)\n",
    "variance= out.var(dim=-1, keepdim=True)\n",
    "print(mean)\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7071,  0.7071],\n",
      "        [ 0.7071, -0.7071]], grad_fn=<DivBackward0>)\n",
      "Normalized Mean:\n",
      " tensor([[-1.4603e-06],\n",
      "        [-2.3842e-07]], grad_fn=<MeanBackward1>)\n",
      "Normalized Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Layer normalization to keep mean =0, and variance =1 \n",
    "# normalz\n",
    "out= (out-mean)/torch.sqrt(variance)\n",
    "print(out) \n",
    "mean= out.mean(dim=-1, keepdim=True)\n",
    "variance= out.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized Mean:\\n\",mean)\n",
    "print(\"Normalized Variance:\\n\", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GELU Activation Function  \n",
    "  - Non traditional activation methods such as GELU and SwiGLU are used to train transformers and LLMs because they offer better performance and smooth activation for gradient descent compared with RELU   \n",
    "    \n",
    "  - GELU uses Gaussian Distribution. It's computed as $x * \\phi(x)$ where $\\phi(x)$ is the cumulative distribution function of a gaussian distribution. It's approximated in implementation. \n",
    "\n",
    "  - At RELU, negative x will have 0 gradient, so it causes problems at gradient descents. But with GELU functions, which are continuous, the function still has gradients with negative x. \n",
    "\n",
    "  - RELU functions also give a non-zero output, so negative inputs can contribute to the function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU (nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "    def forward(self, x): \n",
    "        return 0.5*x *(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi)) * (x+0.044715 * torch.pow(x,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__() \n",
    "        self.layers= nn.Sequential(nn.Linear (cfg[\"embed_dim\"], 4 *cfg[\"embed_dim\"]), \n",
    "                                   GELU(), \n",
    "                                   nn.Linear(4 *cfg[\"embed_dim\"],cfg[\"embed_dim\"])\n",
    "                                   )\n",
    "    def forward(self,x): \n",
    "        return self.layers(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn= Feedforward(cfg=config)\n",
    "x= torch.randn(2,3,768)\n",
    "out= ffn(x) \n",
    "print(out.shape)    # linear layer then GELU activation with the same size then linear layer out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip Connections \n",
    "Skip connections (implemented from ResNet) refer to adding previous layers' outputs to the input of future layers. (Sort of like how our visual cortex passes information beyond a few layers). This allows the model to mitigate vanishing gradient problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "    def forward(self, x): \n",
    "        for layer in self.layers:   # use ModuleList to explicitly show different layers \n",
    "            layer_output= layer(x) # get each layer's output \n",
    "            if self.use_shortcut and x.shape==layer_output.shape:   # add the output layer to x (x will the input to future)\n",
    "                x = x+ layer_output\n",
    "            else: \n",
    "                x=layer_output  # just the output \n",
    "        return x \n",
    "def print_gradients(model,x):\n",
    "    output= model(x) \n",
    "    target= torch.tensor([0.])   # loss is itself \n",
    "    loss= nn.MSELoss() \n",
    "    loss= loss(output, target)\n",
    "    loss.backward() \n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name: \n",
    "            print(f\"{name} has gradient :{param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient :0.00020173587836325169\n",
      "layers.1.0.weight has gradient :0.0001201116101583466\n",
      "layers.2.0.weight has gradient :0.0007152041653171182\n",
      "layers.3.0.weight has gradient :0.001398873864673078\n",
      "layers.4.0.weight has gradient :0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDNN(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient :0.22169792652130127\n",
      "layers.1.0.weight has gradient :0.20694105327129364\n",
      "layers.2.0.weight has gradient :0.32896995544433594\n",
      "layers.3.0.weight has gradient :0.2665732502937317\n",
      "layers.4.0.weight has gradient :1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDNN(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
