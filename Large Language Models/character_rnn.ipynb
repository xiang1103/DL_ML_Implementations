{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import random \n",
    "import unidecode \n",
    "import string \n",
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "\n",
    "torch.backends.cudnn.deterministic = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED= 234 \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE= torch.device('cpu')\n",
    "\n",
    "NUM_ITER= 5000  # epoch size \n",
    "LEARNING_RATE = 0.005 \n",
    "EMBEDDING_DIM= 100  # size of the embedding weight \n",
    "HIDDEN_DIM= 100     # size of the hidden layer \n",
    "NUM_HIDDEN_LAYER= 1     # number of hidden layer after embedding \n",
    "\n",
    "TEXT_PORTION_SIZE= 200  # size of each text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string.printable)    # all the ascii characters this string class can take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84658"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('covid19-faq.txt', 'r') as f:     # open the file \n",
    "    textfile= f.read()  # read the file and load into a variable called textfile \n",
    "\n",
    "textfile = unidecode.unidecode(textfile)    # convert all the special characters \n",
    "\n",
    "# get rid of all the white spaces \n",
    "textfile = re.sub(\" +\",\" \", textfile)\n",
    "\n",
    "TEXT_LENGTH= len(textfile)  # find the total length of the textfile= text_length\n",
    "\n",
    "TEXT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to be submitted. In addition, all current campus health protocols must be followed including wearing facial covering. Additional information on the campus travel policy is located here.\n",
      "\n",
      "Under current\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)    # set the random seed \n",
    "# this will lead to weird texts \n",
    "def random_portion (textfile):  # sample text randomly \n",
    "    start_index= random.randint(0, TEXT_LENGTH-TEXT_PORTION_SIZE)   # randomly sample sentence with TEXT_PORTION_SIZE # of characters in it \n",
    "    end_index= start_index + TEXT_PORTION_SIZE + 1 \n",
    "    return textfile[start_index:end_index]  # return the string of that size \n",
    "\n",
    "print(random_portion(textfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# convert text into torch tensor for training purposes \n",
    "def char_to_tensor(text):\n",
    "    lst= [string.printable.index(c) for c in text]  # convert the text into ASCII characters(ASCII indices) \n",
    "    tensor= torch.tensor(lst).long()   # convert this into a tensor\n",
    "    return tensor \n",
    "print(char_to_tensor(\"abcDEF\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_sample(textfile):\n",
    "    text_long = char_to_tensor(random_portion(textfile))    # randomly get text and convert into torch tensor \n",
    "    input= text_long[:-1]   # input is every letter besides last one \n",
    "    targets= text_long[1:]   # output starts from index 1, we input one and predict the next one (that's why we cut off the last letter of )\n",
    "    return input, targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32, 24, 94, 13, 10, 34, 28, 94, 28, 18, 23, 12, 14, 94, 29, 17, 14, 18,\n",
       "         27, 94, 21, 10, 28, 29, 94, 14, 25, 18, 28, 24, 13, 14, 94, 24, 15, 94,\n",
       "         31, 24, 22, 18, 29, 18, 23, 16, 94, 24, 27, 94, 13, 18, 10, 27, 27, 17,\n",
       "         14, 10, 78, 94, 18, 15, 94, 29, 17, 14, 34, 94, 17, 10, 31, 14, 94, 11,\n",
       "         14, 14, 23, 94, 24, 23, 94, 10, 23, 29, 18, 11, 18, 24, 29, 18, 12, 94,\n",
       "         15, 24, 27, 94, 10, 29, 94, 21, 14, 10, 28, 29, 94,  2,  4, 94, 17, 24,\n",
       "         30, 27, 28, 94, 18, 15, 94, 25, 27, 14, 28, 12, 27, 18, 11, 14, 13, 78,\n",
       "         94, 24, 27, 94, 10, 28, 94, 10, 25, 25, 27, 24, 31, 14, 13, 94, 29, 24,\n",
       "         94, 32, 24, 27, 20, 94, 11, 34, 94, 10, 94, 13, 24, 12, 29, 24, 27, 75,\n",
       "         96, 96, 48, 24, 28, 29, 94, 14, 22, 25, 21, 24, 34, 14, 14, 28, 94, 32,\n",
       "         18, 21, 21, 94, 11, 14, 94, 10, 11, 21, 14, 94, 29, 24, 94, 27, 14, 29,\n",
       "         30, 27]),\n",
       " tensor([24, 94, 13, 10, 34, 28, 94, 28, 18, 23, 12, 14, 94, 29, 17, 14, 18, 27,\n",
       "         94, 21, 10, 28, 29, 94, 14, 25, 18, 28, 24, 13, 14, 94, 24, 15, 94, 31,\n",
       "         24, 22, 18, 29, 18, 23, 16, 94, 24, 27, 94, 13, 18, 10, 27, 27, 17, 14,\n",
       "         10, 78, 94, 18, 15, 94, 29, 17, 14, 34, 94, 17, 10, 31, 14, 94, 11, 14,\n",
       "         14, 23, 94, 24, 23, 94, 10, 23, 29, 18, 11, 18, 24, 29, 18, 12, 94, 15,\n",
       "         24, 27, 94, 10, 29, 94, 21, 14, 10, 28, 29, 94,  2,  4, 94, 17, 24, 30,\n",
       "         27, 28, 94, 18, 15, 94, 25, 27, 14, 28, 12, 27, 18, 11, 14, 13, 78, 94,\n",
       "         24, 27, 94, 10, 28, 94, 10, 25, 25, 27, 24, 31, 14, 13, 94, 29, 24, 94,\n",
       "         32, 24, 27, 20, 94, 11, 34, 94, 10, 94, 13, 24, 12, 29, 24, 27, 75, 96,\n",
       "         96, 48, 24, 28, 29, 94, 14, 22, 25, 21, 24, 34, 14, 14, 28, 94, 32, 18,\n",
       "         21, 21, 94, 11, 14, 94, 10, 11, 21, 14, 94, 29, 24, 94, 27, 14, 29, 30,\n",
       "         27, 23]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_random_sample(textfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__ (self, input_size, embed_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__() \n",
    "        self.hidden_size= hidden_size\n",
    "        self.num_layers= num_layers \n",
    "        self.embed= torch.nn.Embedding(num_embeddings=input_size,embedding_dim=embed_size) # embedding matrix dimension (input size x embedding size(size of dictionary))\n",
    "        # embedding: (batch_szie x 1 x input size) x (input_size x embedding size)= batch_size x 1 x input_size x embedding size\n",
    "        self.rnn= torch.nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers)  # input is embedding size with certain hidden_size and # of hidden layers \n",
    "        # LSTM: (1 x embedding) x (embedding x hidden) = batch_size x 1 x hidden_size  \n",
    "        self.fc= torch.nn.Linear(hidden_size, output_size)  # output into the certain size \n",
    "    \n",
    "    # forward pass, pass in the number of features, and the initial hidden and cell states \n",
    "    def forward(self, features, hidden_cell_state): \n",
    "        # input will be batch_size x 1 for a single character (represented in a single integer)\n",
    "        \n",
    "        # features will be a dimension of [[1]] (features is 1 character size)\n",
    "        features= features.view(1,-1) \n",
    "        \n",
    "        embedded= self.embed(features)  # embedding = 1 x embedding_size \n",
    "\n",
    "        output, hidden_cell_state= self.rnn(embedded, hidden_cell_state)\n",
    "        # output dim: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cell(torch.nn.Module):\n",
    "    # input_size is the vocabulary size (all inputs will be padded into whole vocabulary size)\n",
    "    # output_size is also the vocabulary size\n",
    "    def __init__(self, input_size, embed_size, hidden_size, output_size):\n",
    "        super(). __init__() \n",
    "        self.hidden_size= hidden_size\n",
    "        self.embed= torch.nn.Embedding(num_embeddings=input_size, embedding_dim=embed_size) # embed the input \n",
    "        self.rnn= torch.nn.LSTMCell(input_size= embed_size, hidden_size=hidden_size)    # hidden layers \n",
    "        self.fc= torch.nn.Linear(hidden_size, output_size)  # fully connected layer \n",
    "    \n",
    "    def forward(self, character, hidden, cell_state):\n",
    "        # input are characters, so batch_size x 1 \n",
    "\n",
    "        embedded= self.embed(character)\n",
    "        (hidden, cell_state)= self.rnn(embedded, (hidden, cell_state))  # LSTM detects the input size from embedded, need intial hidden & cell state as the starting point\n",
    "        # output dim: batch_size x output_size \n",
    "        # hidden dimension: batch_size x hidden_dim\n",
    "        # cell/hidden cell dim: batch_size x hidden_dim \n",
    "        \n",
    "        # LSTM cell outputs the next layer's hidden and cell state, we only care about the hidden state\n",
    "        output= self.fc(hidden)\n",
    "        return output, hidden, cell_state\n",
    "    \n",
    "    def init_zero_state(self):  # initialize the original hidden and cell state \n",
    "        hidden= torch.zeros(1,self.hidden_size).to(DEVICE)  # hidden is 1 x hidden_size because batch size is 1 \n",
    "        # batch_size x hidden_size \n",
    "        cell= torch.zeros(1,self.hidden_size).to(DEVICE)    \n",
    "\n",
    "        # initial hidden and cell states are 0s \n",
    "        return (hidden,cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model= RNN_Cell(len(string.printable), EMBEDDING_DIM, HIDDEN_DIM, len(string.printable))\n",
    "model= model.to(DEVICE)\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    ## based on https://github.com/spro/practical-pytorch/\n",
    "    ## blob/master/char-rnn-generation/char-rnn-generation.ipynb\n",
    "\n",
    "    (hidden, cell_state) = model.init_zero_state()\n",
    "    prime_input = char_to_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        inp = prime_input[p].unsqueeze(0)\n",
    "        _, hidden, cell_state = model(inp.to(DEVICE), hidden, cell_state)\n",
    "    inp = prime_input[-1].unsqueeze(0)\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "\n",
    "        outputs, hidden, cell_state = model(inp.to(DEVICE), hidden, cell_state)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = outputs.data.view(-1).div(temperature).exp() # e^{logits / T}\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = string.printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start= time.time() \n",
    "for iter in range (NUM_ITER):\n",
    "    hidden, cell_state= model.init_zero_state() \n",
    "    optimizer.zero_grad()   # clear out gradients \n",
    "    loss=0 \n",
    "    inputs, targets= draw_random_sample(textfile)\n",
    "    inputs, targets= inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "    for c in range(TEXT_PORTION_SIZE):  # input one character at a time and comapre with the actual\n",
    "        outputs, hidden, cell_state= model(inputs[c].unsqueeze(0), hidden, cell_state)      # keep in track of hidden and cell_state for the next character        \n",
    "        loss+= torch.nn.functional.cross_entropy(outputs, targets[c].view(1))\n",
    "    loss /= TEXT_PORTION_SIZE   # find the mean loss \n",
    "    loss.backward() # compute the gradient \n",
    "\n",
    "    optimizer.step() \n",
    "\n",
    "    with torch.no_grad():\n",
    "        if iter % 200 ==0:\n",
    "            print(f\"Time elapsed: {(time.time() - start)/60:.2f} min\")\n",
    "            print(f\"Iteration: {iter}  | Loss: {loss:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
