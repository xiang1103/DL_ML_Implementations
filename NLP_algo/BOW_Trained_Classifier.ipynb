{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "RlUjn91Ga_tJ",
        "outputId": "59e53e4e-4aaa-4876-b892-95c86480b25f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimplementation of the Bag Of Words trained classifer from CMU Advanced NLP class from Fall 2024\\nActual code: https://github.com/neubig/anlp-code/blob/main/01-simpleclassifier/trained_bow_classifier.ipynb\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "implementation of the Bag Of Words trained classifer from CMU Advanced NLP class from Fall 2024\n",
        "Actual code: https://github.com/neubig/anlp-code/blob/main/01-simpleclassifier/trained_bow_classifier.ipynb\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "3 step process to build NLP system:\n",
        "  1. feature extraction function f\n",
        "  2. score calculation by multiplying feature with weights\n",
        "  3. prediction function to make prediction based on the score we calculated (different based on binary class or multi-class prediction)\n",
        "'''"
      ],
      "metadata": {
        "id": "Qtnhvo6XbcYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "Given dataset from Stanford Three Class, data is arranged in \"{groundtruth_score} ||| {sentence}\"\n",
        "\n",
        "1 -> positive\n",
        "0 -> Neutral\n",
        "-1 -> Negative\n",
        "\n",
        "example: \"1 ||| I love this movie\"\n",
        "'''"
      ],
      "metadata": {
        "id": "M8bIldkacFFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "design:\n",
        "  1. read the data\n",
        "  2. extract feature\n",
        "  2*. score calculating function and prediction function\n",
        "  3. training by generating prediction over # of epochs\n",
        "  4. run dev set to generate accuracy\n",
        "  5. error analysis by printing failed cases randomly\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Sis1gjtCcjqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some used libraries\n",
        "import random\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "lfbbgmE_h5QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read the training data file, extract the strings to be trained along with corresponding score\n",
        "def read_xy_data(filename:str) -> tuple[dict[str], dict[int]]:  #tuple because we are returning 2 things\n",
        "  x_data=[]   #strings for training\n",
        "  y_data=[]   #ground truth\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f: #go through every line\n",
        "      label,text= line.strip().split(\" ||| \") #take away white spaces and split the line by the\n",
        "      x_data.append(text)\n",
        "      y_data.append(int(label))\n",
        "  return x_data, y_data"
      ],
      "metadata": {
        "id": "hnJCdD4gdbEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data, y_data= read_xy_data(\"/content/train.txt\")\n",
        "x_test, y_test= read_xy_data(\"/content/dev.txt\")\n"
      ],
      "metadata": {
        "id": "CzW51HD5fvQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract the feature of each word\n",
        "def extract_feature(str)-> dict[str,float]:   #take in a string and output a feature vector(implemented as dictionary) with the feature word and its corresponding weight\n",
        "  features= {}\n",
        "  x_split= str.split(' ') #split the string by space into a list\n",
        "  for word in x_split:\n",
        "    features[word]= features.get(word,0)+1   #update the weight of the feature we just got\n",
        "  return features"
      ],
      "metadata": {
        "id": "yx7_lGxAgHdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weight vector, default all 0\n",
        "feature_weight={}"
      ],
      "metadata": {
        "id": "awaAgg-jiBPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classifier to calculate scores and make predictions\n",
        "def run_classifier(features: dict[str, float]) -> int:  #input feature vector, calculate score and output prediction (1,0,or -1)\n",
        "  score= 0\n",
        "  for feature_str, feature_value in features.items():  #go through every (key,value) pair in features dictionary. (.items() allow us to go through the specific values)\n",
        "    score= score+ feature_value* feature_weight.get(feature_str,0)  #multiply the feature's weight with weight vector\n",
        "  if score >0:\n",
        "    return 1\n",
        "  elif score<0:\n",
        "    return -1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Rj_Ek3JyiDTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training with weight adjustments.\n",
        "  #loop through the training data\n",
        "EPOCHS= 5\n",
        "for epoch in range(0, EPOCHS):\n",
        "  #shuffle all the data\n",
        "  data_ids= list(range(len(x_data)))   #make a list of numbers based on the length of training data (each number is an index)\n",
        "  random.shuffle(data_ids)  #shuffle the list of numbers\n",
        "  for data_id in tqdm.tqdm(data_ids, desc= f\"Epoch: {epoch}\"):\n",
        "    x= x_data[data_id]  #access training material\n",
        "    y=y_data[data_id]   #access groundtruth\n",
        "\n",
        "    #skip over to not train neutral samples\n",
        "    if y==0:\n",
        "      continue\n",
        "\n",
        "    #extract features\n",
        "    features= extract_feature(x)\n",
        "    predict_y= run_classifier(features) #make prediction\n",
        "    if (predict_y!= y): #update the feature weights\n",
        "      for word in features:\n",
        "        feature_weight[word] = feature_weight.get(word,0)+ y*features[word]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFZv-UOUlcFU",
        "outputId": "1b4bab6c-d3fd-4874-9386-6072b3d9e1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 0: 100%|██████████| 8544/8544 [00:00<00:00, 47930.48it/s]\n",
            "Epoch: 1: 100%|██████████| 8544/8544 [00:00<00:00, 46648.21it/s]\n",
            "Epoch: 2: 100%|██████████| 8544/8544 [00:00<00:00, 52053.73it/s]\n",
            "Epoch: 3: 100%|██████████| 8544/8544 [00:00<00:00, 51218.91it/s]\n",
            "Epoch: 4: 100%|██████████| 8544/8544 [00:00<00:00, 53075.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate with test set combined with error analysis\n",
        "error_id=[]   #indices of the error\n",
        "correct_pred=0\n",
        "y_predict=[]\n",
        "total_pred=0\n",
        "for i, (x,y) in enumerate(zip(x_test,y_test)):\n",
        "    features= extract_feature(x)\n",
        "    predict_y= run_classifier(features)\n",
        "    y_predict.append(predict_y) #add predictions for later access\n",
        "    if (predict_y!= y):\n",
        "      error_id.append(i)\n",
        "    else:\n",
        "      correct_pred+=1\n",
        "    total_pred+=1\n",
        "print(f\"Test Accuracy: {correct_pred/ float(total_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyTmzUmM13mW",
        "outputId": "6798c5e2-082d-4000-df33-41cd7403effc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6039963669391463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly print out failed cases\n",
        "for num in range(5):\n",
        "  id= random.choice(error_id)\n",
        "  print(f\"{x_test[id]}\\nprediction: {y_predict[id]}\\ntrue_label: {y_test[id]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWJqwzbS4gWv",
        "outputId": "959b363d-f813-43ea-bf2d-f1290e38c197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "His last movie was poetically romantic and full of indelible images , but his latest has nothing going for it .\n",
            "prediction: 1\n",
            "true_label: -1\n",
            "It takes talent to make a lifeless movie about the most heinous man who ever lived .\n",
            "prediction: 1\n",
            "true_label: -1\n",
            "Not for the prurient or squeamish , it 's a daring if overlong examination of an idolized culture , self-loathing and sexual politics .\n",
            "prediction: 1\n",
            "true_label: 0\n",
            "Returning aggressively to his formula of dimwitted comedy and even dimmer characters , Sandler , who also executive produces , has made a film that makes previous vehicles look smart and sassy .\n",
            "prediction: 1\n",
            "true_label: 0\n",
            "If Steven Soderbergh 's ` Solaris ' is a failure it is a glorious failure .\n",
            "prediction: -1\n",
            "true_label: 1\n"
          ]
        }
      ]
    }
  ]
}