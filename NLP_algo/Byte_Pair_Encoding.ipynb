{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Byte Pair Encoding (BPE)\n",
        "Source code: https://github.com/neubig/anlp-code/blob/main/02-subwords/bpe.ipynb\n"
      ],
      "metadata": {
        "id": "vVKrpsPc9V3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encodings tackle the problem of conjugation in Language processing. In the Bag Of Words method, words are alike sparse vectors and its frequencies. Weights of the exact words are adjusted during training. That means, the conjugation or differen tense of the same word's weight would not be updated until they are being trained.\n",
        "\n",
        "To help models recognize conjugations, one way done by Sennrich et.al (2015) is to **split** a word into different subwords\n",
        "  \n",
        "*ex: expanding --> expand_ing*\n",
        "\n",
        "This is known as tokenization\n"
      ],
      "metadata": {
        "id": "JC6cdybT9qnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methodology**"
      ],
      "metadata": {
        "id": "cW6-aY2yBdzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE takes in a dictionary of words and their corresponding frequencies, and finds the most frequent letter pairs (hence the name Pair Encoding)."
      ],
      "metadata": {
        "id": "jlU1qOBl_UIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loop through the input dictionary and break down the words into characters\n",
        "2. Track the frequency of every two characters and incease their frequency in the output dictionary\n",
        "3. Sort the frequency in decreasing order in terms of frequency\n",
        "4. Embed the vocabulary with the highest occuring pair"
      ],
      "metadata": {
        "id": "ah8Ao1seC7ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuition\n",
        "\n",
        "Conjugated words are similar with the non-conjugated words. If we can find a way to group the words in a way that the more common (non-conjugated) words can be grouped together, then the machine can process the conjugated words similar with the conjugated version. BPE tries to accomplish that.\n",
        "\n",
        "Start off by breaking every word into characters and find the most frequent pairs, then combine the most frequent pairs (known as forming vocabularies), repeat this process until we don't have any new vocabularies to create or stop at a certain threshold.\n",
        "\n",
        "By then, the conjugated words would have grouped the non-conjugated words, the machine will interpret them as the same word"
      ],
      "metadata": {
        "id": "HKOaUrJMQ3n4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GUOZPnM9Rd7"
      },
      "outputs": [],
      "source": [
        "import collections  #collection library is used to initialize the return dictionary\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_pair** counts the frequency of every pair of characters. if a word is already combined, won't have a pair."
      ],
      "metadata": {
        "id": "6MzgQrdVD91e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pair(vocab: dict[str,int]) -> dict[tuple[str,str],int]:\n",
        "  pairs=collections.defaultdict(int)  #output dictionary\n",
        "  for word, freq in vocab.items():\n",
        "    chars= word.split() #split every single character\n",
        "    for i in range(0, len(chars)-1):\n",
        "      pairs[chars[i],chars[i+1]] += freq  #add the frequency to the word pair\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "lYiz6CWMDzV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**merge_vocab** merges all the words with the most frequent pair. Words with the pair in it will be combined, words without the pair has no changes"
      ],
      "metadata": {
        "id": "HfYeWrI2Iq57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(best: tuple[str,str], v_in:dict[str,int])-> dict[str,int]:\n",
        "  v_out={}  #output list\n",
        "  bigram= re.escape(' '.join(best))   #make the pair of two string into one string separated by ' '\n",
        "  p= re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  #make the pair into a pattern detectable in the words\n",
        "  for word in v_in:\n",
        "    word_out= p.sub(''.join(best),word)\n",
        "    v_out[word_out]= v_in[word]\n",
        "  return v_out"
      ],
      "metadata": {
        "id": "3eAvU2qQFxU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the code"
      ],
      "metadata": {
        "id": "IA4hkMI0L_Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\n",
        "    'l o w </w>' : 5,\n",
        "    'l o w e r </w>' : 2,\n",
        "    'l o w e s t </w>':2,\n",
        "    'l o w l y </w>':5,\n",
        "    'w i d e </w>':2\n",
        "}\n",
        "\n",
        "num_ite=10\n",
        "for i in range(0,num_ite):\n",
        "  #print all the pairs\n",
        "  print(f\"{vocab=}\")\n",
        "  pair= get_pair(vocab)\n",
        "  #print(f\"{pair.items()}\")\n",
        "  top_pair= sorted(list(pair.items()), key=lambda x:x[1], reverse=True)[:5] #sort the pairs by frequency\n",
        "  print(f\"{top_pair=}\")\n",
        "  vocab= merge_vocab(top_pair[0][0], vocab)#merge vocabs\n",
        "  print(f\"Merge done\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGhGZHFGL-F3",
        "outputId": "776770cc-d6fe-4a41-be61-1fd16cf16c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab={'l o w </w>': 5, 'l o w e r </w>': 2, 'l o w e s t </w>': 2, 'l o w l y </w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('l', 'o'), 14), (('o', 'w'), 14), (('w', '</w>'), 5), (('w', 'l'), 5), (('l', 'y'), 5)]\n",
            "Merge done\n",
            "\n",
            "vocab={'lo w </w>': 5, 'lo w e r </w>': 2, 'lo w e s t </w>': 2, 'lo w l y </w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('lo', 'w'), 14), (('w', '</w>'), 5), (('w', 'l'), 5), (('l', 'y'), 5), (('y', '</w>'), 5)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low </w>': 5, 'low e r </w>': 2, 'low e s t </w>': 2, 'low l y </w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('low', '</w>'), 5), (('low', 'l'), 5), (('l', 'y'), 5), (('y', '</w>'), 5), (('low', 'e'), 4)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'low e r </w>': 2, 'low e s t </w>': 2, 'low l y </w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('low', 'l'), 5), (('l', 'y'), 5), (('y', '</w>'), 5), (('low', 'e'), 4), (('e', 'r'), 2)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'low e r </w>': 2, 'low e s t </w>': 2, 'lowl y </w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('lowl', 'y'), 5), (('y', '</w>'), 5), (('low', 'e'), 4), (('e', 'r'), 2), (('r', '</w>'), 2)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'low e r </w>': 2, 'low e s t </w>': 2, 'lowly </w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('lowly', '</w>'), 5), (('low', 'e'), 4), (('e', 'r'), 2), (('r', '</w>'), 2), (('e', 's'), 2)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'low e r </w>': 2, 'low e s t </w>': 2, 'lowly</w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('low', 'e'), 4), (('e', 'r'), 2), (('r', '</w>'), 2), (('e', 's'), 2), (('s', 't'), 2)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'lowe r </w>': 2, 'lowe s t </w>': 2, 'lowly</w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('lowe', 'r'), 2), (('r', '</w>'), 2), (('lowe', 's'), 2), (('s', 't'), 2), (('t', '</w>'), 2)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'lower </w>': 2, 'lowe s t </w>': 2, 'lowly</w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('lower', '</w>'), 2), (('lowe', 's'), 2), (('s', 't'), 2), (('t', '</w>'), 2), (('w', 'i'), 2)]\n",
            "Merge done\n",
            "\n",
            "vocab={'low</w>': 5, 'lower</w>': 2, 'lowe s t </w>': 2, 'lowly</w>': 5, 'w i d e </w>': 2}\n",
            "top_pair=[(('lowe', 's'), 2), (('s', 't'), 2), (('t', '</w>'), 2), (('w', 'i'), 2), (('i', 'd'), 2)]\n",
            "Merge done\n",
            "\n"
          ]
        }
      ]
    }
  ]
}