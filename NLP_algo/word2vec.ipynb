{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Word2Vec through Skip Gram and CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skip_gram_neg_sampling (torch.nn.Module):\n",
    "    def __init__ (self, embedding_size, vocab_size, device, negative_samples=10):\n",
    "        super().__init__()  # parent class \n",
    "        self.embedding_size= embedding_size \n",
    "        self.vocab_size= vocab_size \n",
    "        self.device= device \n",
    "        self.neg_samples= negative_samples\n",
    "\n",
    "        # embbeding matrix for the central word \n",
    "        self.embedding_input= torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.context_embedding= torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "\n",
    "        # normalize the embedding weights to be bounded [-1,1]\n",
    "        self.embedding_input.weight.data.uniform_(-1,1)\n",
    "        self.context_embedding.weight.data.uniform_(-1,1)\n",
    "\n",
    "    # skip gram models require the central word and predict context words \n",
    "    def forward(self, input_word, context_word):\n",
    "        # shape is batch_size x hidden_dim\n",
    "        embed_input= self.embedding_input(input_word)   # embed input word \n",
    "        embed_context= self.context_embedding(context_word)\n",
    "        embed_product= torch.mul(embed_input, embed_context)    # multiply the weight matrices \n",
    "\n",
    "        embed_product= torch.sum(embed_product, dim=1)  # sum across hidden dimensions \n",
    "\n",
    "        out_loss= F.logsigmoid(embed_product)   # compute loss\n",
    "\n",
    "        # negative sampling has additional loss terms \n",
    "        # generate random noise -> get # samples -> generate random word --> context embed --> condense to the right size \n",
    "        \n",
    "        noise_dist= torch.ones(self.vocab_size) # generate noise\n",
    "        if self.neg_samples>0: \n",
    "            num_samples= context_word.shape[0]*  self.neg_samples    # find # of negative samples \n",
    "            negative_samples= torch.multinomial(noise_dist,num_samples=num_samples, replacement=True)   \n",
    "            negative_samples= negative_samples.view(context_word.shape[0],self.neg_samples).to(self.device) # bs x num_neg samples \n",
    "\n",
    "            embed_neg = self.embedding_size(negative_samples)   # batch_size x num_neg samples x embed dimension\n",
    "            embed_neg_product= torch.bmm(embed_neg.neg(), embed_input.unsqueeze(2)) # batch_size x num samples x 1  \n",
    "\n",
    "            noise_loss= F.logsigmoid(embed_neg_product).squeeze(2).sum(1)   # batch_size\n",
    "\n",
    "            total_loss= - (out_loss+noise_loss).mean()\n",
    "\n",
    "            return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
