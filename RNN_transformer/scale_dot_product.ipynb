{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the scaled dot product attention with Wq,Wk,Wv matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute key, value, queue values --> compute attention score for each of the words to measure similarity/importance --> scale the attention score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= inputs[1] \n",
    "d_in = inputs. shape[1]     # input dimension \n",
    "d_out= 2    # hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out))   # weight matrices are vocab size x hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "query_2= x @ W_query \n",
    "key_2= x @ W_key \n",
    "value_2= x @W_value \n",
    "\n",
    "# output: 1 x 2 to get the value for query, key and value \n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]], grad_fn=<MmBackward0>)\n",
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# for each input, we need the attention from other inputs of Wk, and Wv \n",
    "keys= inputs @ W_key \n",
    "values= inputs @ W_key  # dim: 6 x 2 \n",
    "print (keys)\n",
    "print (keys.shape)\n",
    "\n",
    "# each row is the key/value of each word (including our current target word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the attention score**  \n",
    "-  Each word has an attention score for every input word. For example, $\\omega_{22}$ is the second word's key value given the target word is the 2nd word \n",
    "-  Compute the dot product between key (of non-input) and the query of input so we get the similarity between these 2 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sample for W22 \n",
    "\n",
    "keys_2= keys[1] # retrive the key value for the second word \n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute this attention score between the input word and every other word in the input sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores= query_2 @ keys.T \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the attention by dividing each term by d_k ** 0.5, so we avoid too large dot products that lead to very small gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_k= keys.shape[-1] # column size= 2 \n",
    "attn_weights= torch.softmax(attn_scores/ d_k **0.5, dim=-1)\n",
    "print (attn_weights)    # 1x 6 \n",
    "\n",
    "# the attention score for each word related to the input word because we multiplied this dot product with input word's query vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the context vector to get the relative importance   \n",
    "-  multiply the relative attention score with corresponding value block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3590, 0.9117], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector= attn_weights @ values # dim: 1x 2 \n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this idea to a scaled dot product attention class where the operations are performed on each row. So instead of 1 x2 context vector for a single row(single input word), we'll have 6x2. Operations are done on each row."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
