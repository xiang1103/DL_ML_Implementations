{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Convolutional Variational Autoencoder at MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_data import get_dataloaders_mnist\n",
    "from helper_utils import set_deterministic, set_all_seeds\n",
    "from helper_plotting import plot_training_loss\n",
    "from helper_plotting import plot_generated_images\n",
    "from helper_plotting import plot_latent_space_with_labels\n",
    "from helper_plotting import plot_images_sampled_from_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "RANDOM_SEED= 123 \n",
    "LEARNING_RATE= 0.0005 \n",
    "BATCH_SIZE=32 \n",
    "NUM_EPOCHS= 20 \n",
    "\n",
    "set_deterministic\n",
    "set_all_seeds(RANDOM_SEED)  #set random seed so we can reproduce the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset from MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\n",
      "Image batch dimension: torch.Size([32, 1, 28, 28])\n",
      "Image label dimension: torch.Size([32])\n",
      "tensor([1, 2, 1, 9, 0, 6, 9, 8, 0, 1])\n",
      "\n",
      "Validation set:\n",
      "\n",
      "Test set:\n",
      "Image batch dimension: torch.Size([32, 1, 28, 28])\n",
      "Image label dimension: torch.Size([32])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader= get_dataloaders_mnist(\n",
    "    batch_size= BATCH_SIZE, num_workers= 2, validation_fraction=0)\n",
    "\n",
    "print(\"Training set:\\n\")\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Image batch dimension: {images.size()}\")\n",
    "    print(f\"Image label dimension: {labels.size()}\")\n",
    "    print(labels[:10])\n",
    "    break \n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "for images, labels in valid_loader:\n",
    "    print(f\"Image batch dimension: {images.size()}\")\n",
    "    print(f\"Image label dimension: {labels.size()}\")\n",
    "    print(labels[:10])\n",
    "    break \n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "for images, labels in test_loader:\n",
    "    print(f\"Image batch dimension: {images.size()}\")\n",
    "    print(f\"Image label dimension: {labels.size()}\")\n",
    "    print(labels[:10])\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model \n",
    "- First we have encoder to encode the input to our latent space of certain dimension \n",
    "- We have to create two vectors mean and log variance (this is the sampling part from the forward pass). We use log variance to make learning more stable   \n",
    "- Then decode the latent space back to the input dimension \n",
    "- Calculate reconstruction cost to optimize reconstruction through backpropogation like a regular autoencoder. We also calculate the KL divergence term to make our latent space distribution into a normal distribution  \n",
    "  \n",
    "Transpose convolution size calculation formula: $$output = (input-1) * stride -2 *padding +(kernel\\_size-1)+1$$\n",
    "\n",
    "#### KL Divergence term\n",
    "The difference between VAE and AE is the variance and mean vectors is that we can use to train and make the latent space distribution into a normal distribution, which is better than that of regular AE.  \n",
    "- To help the model learn a normal distribute latent space, we need to use sampling during a forward pass  \n",
    "- We retrieve mean and variance from a forward pass  \n",
    "- Then compute the z as the final output after encoding \n",
    "- $z= \\mu + \\epsilon \\cdot \\sigma$\n",
    "- We need a $\\mu$ (mean) vector and a variance (vector) to compute this formula. This is why we sample these two after encoding before computing this equation  \n",
    "- With log variance $\\log(\\sigma^2)$ as variance, our standard deviation $\\sigma=$ $e^{\\frac{\\log(\\sigma^2)}{2}}$\n",
    "\n",
    "\n",
    "#### Challenges of AE \n",
    "Autoencoders don't have a normal distribution, so it becomes difficult to sample in a balanced way. It's also not centered at (0,0) and not a continuous distribution at higher dimensional latent spaces. These shortcomings make it good at reconstructing, but not at generating new outcome/out of distribution data.  \n",
    "\n",
    "#### What questions VAE tackles \n",
    "The only difference between a VAE and AE (in this implementation) is that VAE has an extract step of training mean and variance vectors after encoding. This step is done inside the latent space, and with the KL divergence term error, the mean and variance vectors tries to reduce to 0 and 1 respectively while training. This is a normal distribution.  \n",
    "\n",
    "**Note**: The KL divergence term: $$-{\\frac{1}{2}} \\cdot \\sum(1+{\\log(\\sigma^2)}-{\\mu^2}-\\sigma^2)$$  \n",
    "reduces to 0 when variance ($\\sigma^2$) =1 and mean ($\\mu$) =0. Which is why it makes latent space a normal distribution.  \n",
    "  \n",
    "The normal distribution is now a continuous distribution and we can sample balancely, so VAE is better at generating new outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__() \n",
    "        self.shape= args    # reshape the input later at latent space\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class Trim(nn.Module):  # used to cut out dimensions to remain the same as input image \n",
    "    def __init__(self,*args):\n",
    "        super().__init__() \n",
    "    def forward(self,x):\n",
    "        return x[:,:,:28,:28]\n",
    "class VAE(nn.Module):   # input 32 x 1 x 28 x 28    32 training samples, 1 channel each, 28 x 28\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "\n",
    "        # encode the \n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Conv2d(1,32,stride=(1,1), kernel_size=(3,3), padding=1), # 32 x 28 x 28 \n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 64, stride=(2,2), kernel_size=(3,3), padding=1),  # 64 x 14 x 14\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64,64, stride=(2,2), kernel_size=(3,3), padding=1 ),   # 64 x 7 x 7\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64,64, stride= (1,1), kernel_size=(3,3),padding=1),    # 64 x 7 x 7\n",
    "            nn.Flatten()    # flatten into a row vector 1x 3136 \n",
    "        )\n",
    "        # get two vectors for mean and variance \n",
    "        self.z_mean= nn.Linear(3136,2)    # linear transformation into latent sapce \n",
    "        self.z_log_var= nn.Linear(3136,2)\n",
    "       \n",
    "        self.decoder= nn.Sequential(\n",
    "            nn.Lienar(2,3136), # linear transformation back into the original dimension\n",
    "            Reshape(-1,64,7,7),  # remake the hidden space output into 64 x 7 x 7 with the rest training examples (same dimension as our last conv layer output)\n",
    "            nn.ConvTranspose2d(64,64,stride=(1,1),kernel_size=(3,3), padding= 1), #keep same channels and expand the input through transpose convolution. Same dimension\n",
    "            nn.LeakyReLU(0.01),   #non-linear transformation after every transpose convolution\n",
    "            nn.ConvTranspose2d(64,64,stride=(2,2), kernel_size=(3,3),padding=1),  #same channel, and expand the input even more. 1 x 64 x 13 x 13\n",
    "            nn.LeakyReLU(0.01),   #continue scaling \n",
    "            nn.ConvTranspose2d(64,32,stride=(2,2), kernel_size=(3,3), padding=0), # scale down to 32 channels, 1 x 32 x 27 x 27\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(32,1,stride= (1,1), kernel_size=(3,3), padding=0), #revert back to 1 x 1 x 29 x 29\n",
    "            Trim(),     #1x 29 x 29 -> 1 x 28 x 28 take the first 28 pixels of the output. Back to the same dimension and channel of the original input\n",
    "            nn.Sigmoid()    # use sigmoid because training data is transformed to [0,1]\n",
    "        )\n",
    "\n",
    "    def encoding_fn(self, x):\n",
    "        x= self.encoder(x) \n",
    "        z_mean, z_log_var= self.z_mean(x), self.z_log_var   # extract mean and variance vector \n",
    "        encoded= self.reparameterize (z_mean, z_log_var)\n",
    "        return encoded \n",
    "    \n",
    "    def reparameterize(self, z_mu, z_log_var):  # sampling the epsilon after encoding \n",
    "        # pass in mean and log_variance (variance)\n",
    "        eps= torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())  \n",
    "        # keep the batch_size to sample all batches \n",
    "\n",
    "        # compute z from formula\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.0)    #standard deviation is e^(log_var/2)\n",
    "        return z \n",
    "\n",
    "    def forward(self,x):\n",
    "        x= self.encoder(x)\n",
    "        z_mean, z_log_var= self.z_mean(x), self.z_log_var   # retrieve 2 vectors before latent space (both created from linear transformation with different weights) to train for normal distribution\n",
    "        encoded= self.reparameterize(z_mean,z_log_var)  # compute z (the actual latent space)\n",
    "        decoded= self.decoder(encoded)  \n",
    "        return encoded, z_mean, z_log_var, decoded \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
