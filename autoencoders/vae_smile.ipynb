{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use VAE to make images smile more \n",
    "Model wasn't trained, so only helper functions and visualizations are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_data import get_dataloaders_celeba\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 123\n",
    "BATCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "custom_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop((128, 128)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_celeba(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_transforms=custom_transforms,\n",
    "    test_transforms=custom_transforms,\n",
    "    num_workers=2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dimension:  torch.Size([5000, 3, 128, 128])\n",
      "Labels Dimension:  torch.Size([5000, 40])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "for image, labels in train_loader:  \n",
    "    print(\"Image Dimension: \", image.shape) # images have 3 channels, cropped to 128 x 128 \n",
    "    print(\"Labels Dimension: \", labels.shape) # ground truth has 40 dimensions of corresponding attributes\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__() \n",
    "        self.shape= args    # reshape the input later at latent space\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class Trim(nn.Module):  # used to cut out dimensions to remain the same as input image \n",
    "    def __init__(self,*args):\n",
    "        super().__init__() \n",
    "    def forward(self,x):\n",
    "        return x[:,:,:28,:28]\n",
    "class VAE(nn.Module):   # input 32 x 1 x 28 x 28    32 training samples, 1 channel each, 28 x 28\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "\n",
    "        # encode the input \n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Conv2d(1,32,stride=(1,1), kernel_size=(3,3), padding=1), # 32 x 28 x 28 \n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 64, stride=(2,2), kernel_size=(3,3), padding=1),  # 64 x 14 x 14\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64,64, stride=(2,2), kernel_size=(3,3), padding=1 ),   # 64 x 7 x 7\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64,64, stride= (1,1), kernel_size=(3,3),padding=1),    # 64 x 7 x 7\n",
    "            nn.Flatten()    # flatten into a row vector 1x 3136 \n",
    "        )\n",
    "        # get two vectors for mean and variance \n",
    "        self.z_mean= nn.Linear(3136,2)    # linear transformation into latent sapce \n",
    "        self.z_log_var= nn.Linear(3136,2)\n",
    "       \n",
    "        self.decoder= nn.Sequential(\n",
    "            nn.Linear(2,3136), # linear transformation back into the original dimension\n",
    "            Reshape(-1,64,7,7),  # remake the hidden space output into 64 x 7 x 7 with the rest training examples (same dimension as our last conv layer output)\n",
    "            nn.ConvTranspose2d(64,64,stride=(1,1),kernel_size=(3,3), padding= 1), #keep same channels and expand the input through transpose convolution. Same dimension\n",
    "            nn.LeakyReLU(0.01),   #non-linear transformation after every transpose convolution\n",
    "            nn.ConvTranspose2d(64,64,stride=(2,2), kernel_size=(3,3),padding=1),  #same channel, and expand the input even more. 1 x 64 x 13 x 13\n",
    "            nn.LeakyReLU(0.01),   #continue scaling \n",
    "            nn.ConvTranspose2d(64,32,stride=(2,2), kernel_size=(3,3), padding=0), # scale down to 32 channels, 1 x 32 x 27 x 27\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(32,1,stride= (1,1), kernel_size=(3,3), padding=0), #revert back to 1 x 1 x 29 x 29\n",
    "            Trim(),     #1x 29 x 29 -> 1 x 28 x 28 take the first 28 pixels of the output. Back to the same dimension and channel of the original input\n",
    "            nn.Sigmoid()    # use sigmoid because training data is transformed to [0,1]\n",
    "        )\n",
    "\n",
    "    def encoding_fn(self, x):   # only encoding (same as returning the z in the forward() function) \n",
    "        x= self.encoder(x) \n",
    "        z_mean, z_log_var= self.z_mean(x), self.z_log_var(x)   # extract mean and variance vector \n",
    "        encoded= self.reparameterize (z_mean, z_log_var)\n",
    "        return encoded \n",
    "    \n",
    "    def reparameterize(self, z_mu, z_log_var):  # sampling the epsilon after encoding \n",
    "        # pass in mean and log_variance (variance)\n",
    "        eps= torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device()) \n",
    "        # keep the batch_size to sample all batches \n",
    "\n",
    "        # compute z from formula\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.0)    #standard deviation is e^(log_var/2)\n",
    "        return z \n",
    "\n",
    "    def forward(self,x):\n",
    "        x= self.encoder(x)\n",
    "        z_mean, z_log_var= self.z_mean(x), self.z_log_var(x)   # retrieve 2 vectors before latent space (both created from linear transformation with different weights) to train for normal distribution\n",
    "        encoded= self.reparameterize(z_mean,z_log_var)  # compute z (the actual latent space)\n",
    "        decoded= self.decoder(encoded)  \n",
    "        return encoded, z_mean, z_log_var, decoded  # we need these 4 vectors for back prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Manipulation\n",
    "- To generate images that smile more, we use the technique of looking at the images that do smile and images that don't. Compute their differences $$z_{new} = z_{orig} + \\alpha \\cdot z_{diff}$$  \n",
    "- The **intuition** is that by finding what pictures that don't smile don't have, we can add it to the images and make them into pictures with smiles\n",
    "\n",
    "#### Procedure \n",
    "1. Find the average encoding of pictures that smile and don't smile \n",
    "2. Compute the difference vector $z_{diff}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Take in a feature id that corresponds to the attribute that we want to calculate (such as the index 31 of smile)\n",
    "Look at all the images from our data loader that has this attribute from the labels \n",
    "Compute their encoding \n",
    "Add it a sum matrix which would then element-wise divide the total number of images taken, that's our average\n",
    "'''\n",
    "# compute the average with and without smile \n",
    "def compute_average_faces(feature_idx, image_dim, data_loader, device=None, encoding_fn=None):\n",
    "\n",
    "    avg_img_with_feat = torch.zeros(image_dim, dtype=torch.float32)\n",
    "    avg_img_without_feat = torch.zeros(image_dim, dtype=torch.float32)\n",
    "\n",
    "    num_img_with_feat = 0\n",
    "    num_images_without_feat = 0\n",
    "\n",
    "    for images, labels in data_loader:  \n",
    "        # find the  column corresponding to the attribute to turn into False/True vector\n",
    "        idx_img_with_feat = labels[:, feature_idx].to(torch.bool)\n",
    "\n",
    "        if encoding_fn is None:\n",
    "            embeddings = images\n",
    "        else:\n",
    "            ####################################\n",
    "            ### Get latent representation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                if device is not None:\n",
    "                    images = images.to(device)\n",
    "                embeddings = encoding_fn(images).to('cpu')\n",
    "            ####################################    \n",
    "\n",
    "        # only take in account of correspoonding rows \n",
    "        avg_img_with_feat += torch.sum(embeddings[idx_img_with_feat], axis=0)\n",
    "        avg_img_without_feat += torch.sum(embeddings[~idx_img_with_feat], axis=0)\n",
    "        \n",
    "        # sum up all the 0 and 1s to get the total \n",
    "        num_img_with_feat += idx_img_with_feat.sum(axis=0)\n",
    "        num_images_without_feat += (~idx_img_with_feat).sum(axis=0)\n",
    "\n",
    "    avg_img_with_feat /= num_img_with_feat\n",
    "    avg_img_without_feat /= num_images_without_feat\n",
    "    \n",
    "    return avg_img_with_feat, avg_img_without_feat\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
