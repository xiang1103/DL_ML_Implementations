{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of gradient descent with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "- We use linear regression formula f= w * x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([1,2,3,4], dtype=np.float32)    #input \n",
    "Y= np.array([2,4,6,8], dtype=np.float32)    # ground_truth\n",
    "w=0.0   #weight parameter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):         #forward pass function to compute output \n",
    "    return w * x \n",
    "\n",
    "def loss(y_hat, y): #calculate loss from model prediction y_hat and actual output y \n",
    "    return ((y_hat-y)**2).mean()    #average MSE error with 1/n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the gradient \n",
    "MSE= 1/N * (w*x-y)^2 \n",
    "\n",
    "Gradient = 1/N * 2(wx-y)*x = dl/dw   \n",
    "\n",
    "Because our parameter w is 1 x 1, so when we take the mean, the gradient of dl/dw goes from 1x4 vector to a 1x1 element. Need to be careful of the gradient dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,y_hat):\n",
    "    return np.multiply(2*x, y_hat-y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training f([1,2,3,4]): [0. 0. 0. 0.]\n",
      "Error before training: 30.0\n",
      "Gradient before training: -30.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction before training f([1,2,3,4]):\", forward(X))\n",
    "error= loss(forward(X), Y)\n",
    "print(\"Error before training:\", error)\n",
    "grad = gradient(X,Y,forward(X))\n",
    "print(\"Gradient before training:\", grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training over iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1:\n",
      " w=1.987, loss= 0.00174685\n",
      "epoch: 2:\n",
      " w=1.989, loss= 0.00126211\n",
      "epoch: 3:\n",
      " w=1.991, loss= 0.00091188\n",
      "epoch: 4:\n",
      " w=1.992, loss= 0.00065882\n",
      "epoch: 5:\n",
      " w=1.993, loss= 0.00047601\n",
      "epoch: 6:\n",
      " w=1.994, loss= 0.00034391\n",
      "epoch: 7:\n",
      " w=1.995, loss= 0.00024848\n",
      "epoch: 8:\n",
      " w=1.996, loss= 0.00017952\n",
      "epoch: 9:\n",
      " w=1.996, loss= 0.00012971\n",
      "epoch: 10:\n",
      " w=1.997, loss= 0.00009371\n",
      "Prediction after training: [1.9969954 3.993991  5.9909863 7.987982 ]\n"
     ]
    }
   ],
   "source": [
    "n_iter= 10 \n",
    "LEARNING_RATE= 0.01 \n",
    "for epoch in range(n_iter):\n",
    "    #generate prediction \n",
    "    y_hat= forward(X)\n",
    "    #loss \n",
    "    error= loss(y_hat, Y)\n",
    "    #gradient \n",
    "    dw= gradient(X,Y,y_hat)\n",
    "    #update weights \n",
    "    w -= LEARNING_RATE* dw  #upate weight to the opposite direction of the gradient (element wise change)\n",
    "    print(f\"epoch: {epoch+1}:\\n w={w:.3f}, loss= {error:.8f}\")\n",
    "print(f\"Prediction after training: {forward(X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch with Gradient Descent using Autograd package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training f([1,2,3,4]): tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>)\n",
      "Error before training: tensor(30., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor([1,2,3,4], dtype=torch.float32)  #require_grad = False by default \n",
    "Y= torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w= torch.tensor(0.0,dtype=torch.float32, requires_grad=True)    #out parameter w \n",
    "\n",
    "# forward pass \n",
    "def forward(x): \n",
    "    return w * x    #element wise multiplication \n",
    "\n",
    "# loss function \n",
    "def loss(y_hat, y):\n",
    "    return ((y_hat-y)**2).mean()    # MSE error\n",
    "\n",
    "print(\"Prediction before training f([1,2,3,4]):\", forward(X))\n",
    "error= loss(forward(X), Y)\n",
    "print(\"Error before training:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1:\n",
      " w=1.987, loss= 0.00174685\n",
      "epoch: 2:\n",
      " w=1.989, loss= 0.00126211\n",
      "epoch: 3:\n",
      " w=1.991, loss= 0.00091188\n",
      "epoch: 4:\n",
      " w=1.992, loss= 0.00065882\n",
      "epoch: 5:\n",
      " w=1.993, loss= 0.00047601\n",
      "epoch: 6:\n",
      " w=1.994, loss= 0.00034392\n",
      "epoch: 7:\n",
      " w=1.995, loss= 0.00024848\n",
      "epoch: 8:\n",
      " w=1.996, loss= 0.00017952\n",
      "epoch: 9:\n",
      " w=1.996, loss= 0.00012971\n",
      "epoch: 10:\n",
      " w=1.997, loss= 0.00009371\n",
      "Prediction after training: tensor([1.9970, 3.9940, 5.9910, 7.9880], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE= 0.01 \n",
    "n_iter=10\n",
    "\n",
    "for epoch in range (n_iter):\n",
    "    #forward pass \n",
    "    y_hat= forward(X)\n",
    "\n",
    "    #calculate loss \n",
    "    error= loss(y_hat, Y)\n",
    "\n",
    "    #calculate gradient with Autograd package \n",
    "    error.backward()    #dw is automatically calculated and stored in w.grad \n",
    "\n",
    "    #update w \n",
    "    with torch.no_grad(): #not included in gradient computation\n",
    "        w-= LEARNING_RATE* w.grad   #gradient auto computed by Autograd\n",
    "    \n",
    "    #empty the gradient of w because they accumulate in error.backward() \n",
    "    w.grad.zero_()\n",
    "\n",
    "    print(f\"epoch: {epoch+1}:\\n w={w:.3f}, loss= {error:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training: {forward(X)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent with Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1:\n",
      " w=0.030, loss= 30.00000000\n",
      "epoch: 2:\n",
      " w=0.060, loss= 29.10675049\n",
      "epoch: 3:\n",
      " w=0.089, loss= 28.24009705\n",
      "epoch: 4:\n",
      " w=0.117, loss= 27.39924622\n",
      "epoch: 5:\n",
      " w=0.146, loss= 26.58343506\n",
      "epoch: 6:\n",
      " w=0.173, loss= 25.79191589\n",
      "epoch: 7:\n",
      " w=0.201, loss= 25.02395821\n",
      "epoch: 8:\n",
      " w=0.228, loss= 24.27886963\n",
      "epoch: 9:\n",
      " w=0.254, loss= 23.55596542\n",
      "epoch: 10:\n",
      " w=0.281, loss= 22.85458755\n",
      "Prediction after training: tensor([0.2805, 0.5611, 0.8416, 1.1222], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor([1,2,3,4], dtype=torch.float32)  #require_grad = False by default \n",
    "Y= torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w= torch.tensor(0.0,dtype=torch.float32, requires_grad=True)    #out parameter w \n",
    "\n",
    "# forward pass \n",
    "def forward(x): \n",
    "    return w * x    #element wise multiplication \n",
    "LEARNING_RATE=0.001\n",
    "n_iter= 10\n",
    "loss= torch.nn.MSELoss() \n",
    "optimizer= torch.optim.SGD([w], lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    #prediction\n",
    "    y_hat= forward(X)\n",
    "    #loss \n",
    "    error= loss(y_hat, Y)\n",
    "\n",
    "    #calculate gradient \n",
    "    error.backward()\n",
    "\n",
    "    #update weight through our optimizer \n",
    "    optimizer.step()    #optimize the gradients \n",
    "\n",
    "    #clear out our gredients \n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    print(f\"epoch: {epoch+1}:\\n w={w:.3f}, loss= {error:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training: {forward(X)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
