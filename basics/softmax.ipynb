{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Softmax layer with Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax \n",
    "- With more than one element output, we use take the exponential of each element and divide the sum to get the probability of each output.  \n",
    "- The element with the highest probability is the softmax layer output \n",
    "- Good at classification tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)  # exponential each element and divide the sum of the exponential. Sum is performed on rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of individual output [0.65900114 0.24243297 0.09856589]\n",
      "Combined Probability:  1.0\n"
     ]
    }
   ],
   "source": [
    "x=np.array([2.0,1.0,0.1])\n",
    "output= softmax(x)\n",
    "print(\"Probability of individual output\",output)   #total will add up to be 1 \n",
    "print(\"Combined Probability: \",np.sum(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "#PyTorch implementation \n",
    "x= torch.tensor([2.0,1.0,0.1])\n",
    "output= torch.softmax(x,dim=0)  #compute the sum along the rows \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy \n",
    "sum of the ground truth * log(y_prediction) * -1    \n",
    "  \n",
    "The further apart prediction is to the ground truth, the higher the loss is.  \n",
    "- Because cross entropy is based on probability distributions where p(x) is the probability distribution of the correct label and log(p(x)) is the probability of prediction. It is best used when the prediction and ground truth is $\\in [0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First loss: 0.356675\n",
      "Second loss: 2.302585\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    loss= -np.sum(y * np.log(y_hat))\n",
    "    return loss \n",
    "\n",
    "Y= np.array([1,0,0])\n",
    "y1= np.array([0.7,0.2,0.1])\n",
    "y2 =np.array([0.1,0.3,0.6])\n",
    "l1= cross_entropy(y1,Y)\n",
    "l2= cross_entropy(y2,Y)\n",
    "\n",
    "print (f\"First loss: {l1:.6f}\")\n",
    "print (f\"Second loss: {l2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch CrossEntropy Loss\n",
    "\n",
    "softmax + negative log likehood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good loss: 0.417030\n",
      "Loss loss: 1.840616\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y= torch.tensor([0])    \n",
    "y_good= torch.tensor([[2.0,1.0,0.1]])# n_samples x n_classes. \n",
    "\n",
    "# Pytorch will apply softmax layer compute the loss sum with the ground truth \n",
    "y_bad= torch.tensor([[0.5,2.0,0.3]])\n",
    "\n",
    "l1= loss(y_good,Y)\n",
    "l2= loss(y_bad, Y)\n",
    "print (f\"Good loss: {l1.item():.6f}\")\n",
    "print (f\"Loss loss: {l2.item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
