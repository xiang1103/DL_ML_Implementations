{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Programming for Policy evaluation and improvement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation is based on the OpenAI gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation --> Bellman Expectation Equation\n",
    "-  Given a policy, evaluate how good a policy is by computing the sum of rewards across all states based on the action from the policy, multiplied with the probability of going there  \n",
    "-  Iterate the process multiple times until convergence because our initial reward could be wrong. So we need to calculate multiple times until the actual correct values stop changing. \n",
    "-  **Notice** that this evaluation procedure will not generate the maximum state-value function\n",
    "$$\n",
    "V^\\pi(s) =  \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration ---> Bellman Optimality Equation \n",
    "-  Tries to find the optimal policy (like value iteration method)\n",
    "-  Split calculating state-value function V and policy calculations \n",
    "-  **Policy Improvement + policy evaluation**\n",
    "-  Computes the value function then update the policy based on the value function. Iteratively doing this step until policy stops changing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration --> Bellman Optimality Equation  \n",
    "-  Try to find the optimal policy through iteratively improving the state-value function until convergence \n",
    "-  Computes the value function and stores the corresponding actions at each state. Once convergence, we get the policy from the actions at each state \n",
    "-  Our sate value function is dependent on the max value generated from a certain action\n",
    "$$\n",
    "V^\\star(s) = \\max_{a}  \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^\\star(s') \\right]\n",
    "$$\n",
    "$$\n",
    "\\pi^\\star(s) = \\argmax_{a}  \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^\\star(s') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m \n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nSFFF            # (S: start)\\n    FHFH            # (F: frozen, safe)\\n    FFFH            # (H: hole, failure)\\n    HFFG            # (G: goal)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() \n",
    "env.render() \n",
    "\n",
    "# the state number is treated as an index on this 4x4 grid, 0-indexed \n",
    "\n",
    "'''  \n",
    "SFFF            # (S: start)\n",
    "    FHFH            # (F: frozen, safe)\n",
    "    FFFH            # (H: hole, failure)\n",
    "    HFFG            # (G: goal)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: 16, actions: 4\n"
     ]
    }
   ],
   "source": [
    "num_states= env.observation_space.n \n",
    "num_actions= env.action_space.n\n",
    "print(f\"states: {num_states}, actions: {num_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n"
     ]
    }
   ],
   "source": [
    "state= env.reset()  # get the current state \n",
    "print(state)    # 100% probability to be started at that state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions: LEFT(0), DOWN(1), UP(2) and RIGHT(3)\n",
    "\n",
    "new_state, reward, is_done, _, info = env.step(3)   # make an action on the right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(new_state)\n",
    "print(reward)   # the reward is 0 because we landed on the hole \n",
    "print(is_done)  # we fell into the hole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n"
     ]
    }
   ],
   "source": [
    "# reset the environment \n",
    "state= env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, is_done, _, info= env.step(1)    # make a step towards the bottom \n",
    "print(new_state)\n",
    "print(info) # returns the transition probability of going to that state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability is stored in a transition matrix, which covers the probability of going to a state s_t from the state s   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(0.3333333333333333, 0, 0.0, False),\n",
      "     (0.3333333333333333, 0, 0.0, False),\n",
      "     (0.3333333333333333, 4, 0.0, False)],\n",
      " 1: [(0.3333333333333333, 0, 0.0, False),\n",
      "     (0.3333333333333333, 4, 0.0, False),\n",
      "     (0.3333333333333333, 1, 0.0, False)],\n",
      " 2: [(0.3333333333333333, 4, 0.0, False),\n",
      "     (0.3333333333333333, 1, 0.0, False),\n",
      "     (0.3333333333333333, 0, 0.0, False)],\n",
      " 3: [(0.3333333333333333, 1, 0.0, False),\n",
      "     (0.3333333333333333, 0, 0.0, False),\n",
      "     (0.3333333333333333, 0, 0.0, False)]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "pprint(env.env.P[0])    # prin the transition matrix at state 0 \n",
    "\n",
    "# the key is the action taken \n",
    "# first value is probability of going to that state, \n",
    "# second value is the resulting state \n",
    "# third value is the reward \n",
    "# fourth value is whether the episode has ended "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing policy evaluation functions   \n",
    "V function is written as  (probability of doing this action) * (reward of the new state + gamma * V of the new state)  \n",
    "\n",
    "-  Note that we take pi[at | st] as the probability of doing this action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold= 0.0001    # value required for convergence \n",
    "gamma= 0.99         # discounted value gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the policy -> V function \n",
    "# we need to find the maximum return from actions available at each state (state-action value )\n",
    "# to do this, we will need to keep track of the return from each state, so we can use (like a prefix sum)\n",
    "# DP comes in from storing these values \n",
    "def policy_eval (env, policy, gamma, threshold):\n",
    "    num_states= env.observation_space.n # get number of states \n",
    "    V= torch.zeros(num_states)  # V function array \n",
    "    max_delta= threshold+1  # \\delta which is the maximum differnece between the return of two state action values (check for conv.)\n",
    "    while max_delta> threshold: \n",
    "        temp= torch.zeros(num_states)   # temperary array to store the biggest values as we go through the actions\n",
    "        for state in range (num_states):    # go through all the possible states (each state is numbered from 0-5, so we can traverse like that)\n",
    "            action= policy[state].item()    # get the action from our policy at each state\n",
    "            for proba, new_state, reward, _ in env.env.P[state][action]:    # retrive the information at a given state and apply a given function (from the transition matrix)\n",
    "                temp[state]+=proba* (reward + gamma * V[new_state]) # compute the temp V value\n",
    "        max_delta= torch.max(torch.abs(V-temp)) # get the difference of the old value and the new value adn get the difference\n",
    "        V= temp.clone()     # make V into the temp \n",
    "    return V    # when the difference between new policy actions and the original is smaller than convergence term, our policy have converged  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "Finding the best action to take at each state inside the policy function, so that we get the most reward   \n",
    "\n",
    "Go through all the states, for each action, simulate to get the corresponding reward, add them up and find the best action for each state the policy might face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement (env, V, gamma):\n",
    "    num_states= env.observation_space.n\n",
    "    num_actions= env.action_space.n # number of possible actions at each state \n",
    "    policy= torch.zeors (num_states)    # our policy stores an action at each state (we want to optimize this)\n",
    "    for state in range (num_states):\n",
    "        actions= torch.zeros(num_actions)   # a table of actions to keep in track the reward from each action, so we can get the most optimal action\n",
    "        for action in range(num_actions):   # iterate through each action to compute the reward\n",
    "            # evaluate the reward \n",
    "            for proba, new_state, reward, _ in env.env.P[state][action]: \n",
    "                actions[action] +=proba * (reward + gamma* V[new_state])\n",
    "        policy[state]= torch.argmax(actions)\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that in both policy evaluation and improvement function, we add up the temp function from each state, this is the DP/summation part,because we need to sump up all the values from these states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration   \n",
    "\n",
    "Improves the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration (env, gamma=gamma, threshold= threshold):\n",
    "    num_states= env.observation_space.n \n",
    "    num_actions= env.action_space.n \n",
    "    policy= torch.randint(low=0, high=num_actions,size=(num_actions,)).float() # create random actions for initial policy\n",
    "    while True: \n",
    "        V=policy_eval(env, policy, gamma=gamma, threshold=threshold)\n",
    "        new_policy= policy_improvement(env, V,gamma=gamma)\n",
    "        if torch.equal(new_policy, policy):\n",
    "            return V, new_policy\n",
    "        policy= new_policy.clone() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration  \n",
    "Finds the optimal reward at each state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, thres = 0.0001):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    V = torch.zeros(num_states)\n",
    "    max_delta = thres + 1\n",
    "    while max_delta > thres:\n",
    "        temp = torch.zeros(num_states)\n",
    "        for state in range(num_states):\n",
    "            v_actions = torch.zeros(num_actions)\n",
    "            for action in range(num_actions):\n",
    "                for proba, new_state, reward, is_done in env.env.P[state][action]:\n",
    "                    v_actions[action] += proba * (reward + gamma * V[new_state])    # Value iteration \n",
    "            temp[state] = torch.max(v_actions)              # Select the action with the highest reward\n",
    "        max_delta = torch.max(torch.abs(V - temp))\n",
    "        V = temp.clone()\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optimal_policy(env, V, gamma=0.99):\n",
    "    num_states, num_actions = env.observation_space.n, env.action_space.n\n",
    "    optimal_policy = torch.zeros(num_states)\n",
    "    for state in range(num_states):\n",
    "        v_actions = torch.zeros(num_actions)\n",
    "        for action in range(num_actions):\n",
    "            for proba, new_state, reward, _ in env.env.P[state][action]:\n",
    "                v_actions[action] += proba * (reward + gamma * V[new_state])\n",
    "        optimal_policy[state] = torch.argmax(v_actions)\n",
    "    return optimal_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
